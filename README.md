# Databricks_end-to-end_ETL_AWS
<img width="723" alt="Project_architecture" src="https://github.com/pranavarora99/Databricks_end-to-end_ETL_AWs/assets/43396873/2c9d05ed-f2c1-431c-a1ee-b6d5f0d2388c">
1. created an AWS S3 Bucket and Upload data
2. Created a Databricks workspace and cluster
3. Processed the data in Databricks
    * In the Databricks workspace, created a new notebook.
    * Read the data from S3 and perform data processing operations.
4. Transformed the data using Apache Spark
    * Used Apache Spark APIs to transform the data into the desired format.
    * Transformed data to S3.
5. Loaded the data into Snowflake
    * Used Apache Spark APIs to transform the data into the desired format.
    * Transformed data to S3.
